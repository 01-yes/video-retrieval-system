# test_clip.py
import open_clip
import torch
import numpy as np
from PIL import Image
import sys
import os

def test_clip_functionality():
    """æµ‹è¯•CLIPæ¨¡å‹åŠŸèƒ½"""
    print("=" * 50)
    print("        CLIP åŠŸèƒ½æµ‹è¯•")
    print("=" * 50)
    
    try:
        # 1. å¯¼å…¥æµ‹è¯•
        print("1. å¯¼å…¥open_clip...")
        print("âœ“ open_clip å¯¼å…¥æˆåŠŸ")
        print("ç‰ˆæœ¬:", open_clip.__version__)
        
        # 2. åŠ è½½æ¨¡å‹
        print("\n2. åŠ è½½CLIPæ¨¡å‹...")
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')
        print("âœ“ CLIPæ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # 3. æ£€æŸ¥è®¾å¤‡
        print("\n3. æ£€æŸ¥è®¾å¤‡...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print("è®¾å¤‡:", device)
        model.to(device)
        
        # 4. æµ‹è¯•å›¾åƒç¼–ç å™¨
        print("\n4. æµ‹è¯•å›¾åƒç¼–ç å™¨...")
        # åˆ›å»ºä¸€ä¸ªéšæœºå›¾åƒè¿›è¡Œæµ‹è¯•
        random_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        image_tensor = preprocess(random_image).unsqueeze(0).to(device)
        
        with torch.no_grad():
            image_features = model.encode_image(image_tensor)
            print("âœ“ å›¾åƒç¼–ç å™¨å·¥ä½œæ­£å¸¸")
            print("ç‰¹å¾ç»´åº¦:", image_features.shape)
        
        # 5. æµ‹è¯•æ–‡æœ¬ç¼–ç å™¨
        print("\n5. æµ‹è¯•æ–‡æœ¬ç¼–ç å™¨...")
        text_tokens = open_clip.tokenize(["a photo of a cat", "a picture of a dog"]).to(device)
        
        with torch.no_grad():
            text_features = model.encode_text(text_tokens)
            print("âœ“ æ–‡æœ¬ç¼–ç å™¨å·¥ä½œæ­£å¸¸")
            print("æ–‡æœ¬ç‰¹å¾ç»´åº¦:", text_features.shape)
        
        # 6. æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—
        print("\n6. æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—...")
        image_features = image_features / image_features.norm(dim=-1, keepdim=True)
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        print("âœ“ ç›¸ä¼¼åº¦è®¡ç®—æ­£å¸¸")
        print("å›¾åƒ-æ–‡æœ¬ç›¸ä¼¼åº¦:", similarity.cpu().numpy())
        
        # 7. æ€§èƒ½æµ‹è¯•
        print("\n7. æ€§èƒ½æµ‹è¯•...")
        import time
        
        # æµ‹è¯•å¤„ç†é€Ÿåº¦
        start_time = time.time()
        test_images = 10
        
        for i in range(test_images):
            test_img = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
            img_tensor = preprocess(test_img).unsqueeze(0).to(device)
            with torch.no_grad():
                _ = model.encode_image(img_tensor)
        
        end_time = time.time()
        avg_time = (end_time - start_time) / test_images
        print(f"å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.3f} ç§’/å›¾åƒ")
        print(f"é¢„ä¼°FPS: {1/avg_time:.1f}")
        
        print("\n" + "=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CLIPå®Œå…¨å¯ç”¨")
        print("=" * 50)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print("é”™è¯¯ç±»å‹:", type(e).__name__)
        
        # æä¾›å…·ä½“çš„é”™è¯¯è§£å†³æ–¹æ¡ˆ
        if "CUDA" in str(e):
            print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ: å°è¯•ä½¿ç”¨CPUæ¨¡å¼æˆ–æ£€æŸ¥CUDAå®‰è£…")
        elif "download" in str(e).lower():
            print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ: æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œæˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹")
        elif "module" in str(e).lower():
            print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ: é‡æ–°å®‰è£…ä¾èµ–: pip install open-clip-torch")
        
        return False

def test_clip_with_real_image(image_path=None):
    """ä½¿ç”¨çœŸå®å›¾åƒæµ‹è¯•CLIP"""
    print("\n" + "=" * 50)
    print("     çœŸå®å›¾åƒCLIPæµ‹è¯•")
    print("=" * 50)
    
    try:
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model.to(device)
        
        if image_path and os.path.exists(image_path):
            # ä½¿ç”¨æä¾›çš„å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            print(f"æµ‹è¯•å›¾åƒ: {os.path.basename(image_path)}")
        else:
            # åˆ›å»ºæµ‹è¯•å›¾åƒ
            print("ä½¿ç”¨ç”Ÿæˆçš„æµ‹è¯•å›¾åƒ")
            image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        
        # é¢„å¤„ç†å›¾åƒ
        image_tensor = preprocess(image).unsqueeze(0).to(device)
        
        # æå–ç‰¹å¾
        with torch.no_grad():
            features = model.encode_image(image_tensor)
            features = features.cpu().numpy().flatten()
        
        print(f"âœ“ ç‰¹å¾æå–æˆåŠŸ")
        print(f"ç‰¹å¾å‘é‡ç»´åº¦: {features.shape}")
        print(f"ç‰¹å¾èŒƒå›´: [{features.min():.3f}, {features.max():.3f}]")
        print(f"ç‰¹å¾èŒƒæ•°: {np.linalg.norm(features):.3f}")
        
        # æµ‹è¯•æ–‡æœ¬åŒ¹é…
        texts = [
            "a photo of an animal",
            "a picture of a landscape", 
            "an image of a person",
            "a graphic design",
            "a random pattern"
        ]
        
        text_tokens = open_clip.tokenize(texts).to(device)
        
        with torch.no_grad():
            text_features = model.encode_text(text_tokens)
            # å½’ä¸€åŒ–
            image_features_norm = features / np.linalg.norm(features)
            text_features_norm = text_features / text_features.norm(dim=-1, keepdim=True)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarities = (image_features_norm @ text_features_norm.cpu().numpy().T).flatten()
        
        print(f"\nğŸ“Š æ–‡æœ¬åŒ¹é…ç»“æœ:")
        for i, (text, sim) in enumerate(zip(texts, similarities)):
            print(f"  {i+1}. '{text}' -> ç›¸ä¼¼åº¦: {sim:.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çœŸå®å›¾åƒæµ‹è¯•å¤±è´¥: {e}")
        return False

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–åŒ…"""
    print("=" * 50)
    print("       ä¾èµ–åŒ…æ£€æŸ¥")
    print("=" * 50)
    
    dependencies = {
        'torch': 'æ·±åº¦å­¦ä¹ æ¡†æ¶',
        'open_clip': 'CLIPæ¨¡å‹',
        'PIL': 'å›¾åƒå¤„ç†',
        'numpy': 'æ•°å€¼è®¡ç®—'
    }
    
    all_ok = True
    for package, description in dependencies.items():
        try:
            if package == 'PIL':
                __import__('PIL.Image')
            else:
                __import__(package)
            print(f"âœ“ {package:15} - {description}")
        except ImportError:
            print(f"âŒ {package:15} - æœªå®‰è£…")
            all_ok = False
    
    return all_ok

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¬ CLIPåŠŸèƒ½å®Œæ•´æ€§æµ‹è¯•")
    print("æ­¤è„šæœ¬å°†æµ‹è¯•CLIPæ¨¡å‹çš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½")
    print()
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print("\nâŒ ç¼ºå°‘ä¾èµ–åŒ…ï¼Œè¯·å…ˆå®‰è£…:")
        print("pip install open-clip-torch torch torchvision pillow numpy")
        return
    
    print("\nå¼€å§‹CLIPåŠŸèƒ½æµ‹è¯•...")
    
    # è¿è¡ŒåŸºæœ¬åŠŸèƒ½æµ‹è¯•
    basic_ok = test_clip_functionality()
    
    # è¿è¡ŒçœŸå®å›¾åƒæµ‹è¯•
    real_image_ok = test_clip_with_real_image()
    
    print("\n" + "=" * 60)
    print("             æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    
    if basic_ok and real_image_ok:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CLIPå¯ä»¥æ­£å¸¸ä½¿ç”¨")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. è¿è¡Œ demo.py è¿›è¡Œç³»ç»Ÿæ¼”ç¤º")
        print("2. è¿è¡Œ main.py ä½¿ç”¨å®Œæ•´ç³»ç»Ÿ")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é”™è¯¯ä¿¡æ¯")
        print("\nå¸¸è§è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("2. é‡æ–°å®‰è£…ä¾èµ–: pip install --upgrade open-clip-torch")
        print("3. æ£€æŸ¥CUDAå®‰è£…ï¼ˆå¦‚ä½¿ç”¨GPUï¼‰")

if __name__ == "__main__":
    main()